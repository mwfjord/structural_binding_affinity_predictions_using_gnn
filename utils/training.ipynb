{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# !conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia\n",
    "# !conda install pyg -c pyg\n",
    "# !conda install gemmi -c conda-forge\n",
    "# !conda install biopython\n",
    "# !pip install pyg-lib torch-scatter torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}+${CUDA}.html\n",
    "# !pip install torch-geometric\n",
    "# !pip install wandb\n",
    "# !pip install plotly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.4.0+cu124\n",
      "CUDA available:  True\n",
      "Torch geometric version: 2.6.1\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmwfjord\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "structural_binding_affinity_predictions_using_gnn\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/marcus/programming/structural_binding_affinity_predictions_using_gnn/src/wandb/run-20250207_171140-25yve2ja</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mwfjord/structural_binding_affinity_predictions_using_gnn/runs/25yve2ja' target=\"_blank\">GNN Training</a></strong> to <a href='https://wandb.ai/mwfjord/structural_binding_affinity_predictions_using_gnn' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mwfjord/structural_binding_affinity_predictions_using_gnn' target=\"_blank\">https://wandb.ai/mwfjord/structural_binding_affinity_predictions_using_gnn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mwfjord/structural_binding_affinity_predictions_using_gnn/runs/25yve2ja' target=\"_blank\">https://wandb.ai/mwfjord/structural_binding_affinity_predictions_using_gnn/runs/25yve2ja</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/mwfjord/structural_binding_affinity_predictions_using_gnn/runs/25yve2ja?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x784340339b70>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marcus/programming/structural_binding_affinity_predictions_using_gnn/src/proteinDNADataset.py:138: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(os.path.join(self.processed_dir,\n",
      "/home/marcus/programming/gnn-protein-dna/lib/python3.10/site-packages/torch_geometric/data/dataset.py:169: UserWarning: Found floating-point labels while calling `dataset.num_classes`. Returning the number of unique elements. Please make sure that this is expected before proceeding.\n",
      "  warnings.warn(\"Found floating-point labels while calling \"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./../data)... Done. 1.5s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Artifact KLF1_K_d>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = config.training.batch_size\n",
    "\n",
    "# Load preprocessed datasets\n",
    "train_dataset = ProteinDNADataset(\"../data\")\n",
    "val_dataset = ProteinDNADataset(\"../data\", validation=True)\n",
    "test_dataset = ProteinDNADataset(\"../data\", test=True)\n",
    "\n",
    "# print(\"Training sample:\", train_dataset[0])\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "data_details = {\n",
    "    \"num_node_features\": train_dataset.num_node_features,\n",
    "    \"num_edge_features\": train_dataset.num_edge_features,\n",
    "    \"num_classes\": train_dataset.num_classes,\n",
    "    \"num_training_samples\": len(train_dataset),\n",
    "    \"num_validation_samples\": len(val_dataset),\n",
    "    \"num_test_samples\": len(test_dataset),\n",
    "}\n",
    "\n",
    "\n",
    "# Log the dataset to W&B as an artifact.\n",
    "dataset_artifact = wandb.Artifact(name=\"KLF1_K_d\", type=\"dataset\", metadata=data_details)\n",
    "dataset_artifact.add_dir(\"../data\")\n",
    "wandb.log_artifact(dataset_artifact)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "Using GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marcus/programming/structural_binding_affinity_predictions_using_gnn/src/proteinDNADataset.py:138: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(os.path.join(self.processed_dir,\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 432523544.8889, Val Loss = 450496608.0000\n",
      "Epoch 10: Train Loss = 429904672.0000, Val Loss = 450488096.0000\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▃▃▃▄▅▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇██▁</td></tr><tr><td>train/loss</td><td>██▇▇▆▃▂▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁</td></tr><tr><td>val/loss</td><td>█▇▇▅▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>train/loss</td><td>429904672.0</td></tr><tr><td>val/loss</td><td>450488096.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">GNN Training</strong> at: <a href='https://wandb.ai/mwfjord/structural_binding_affinity_predictions_using_gnn/runs/25yve2ja' target=\"_blank\">https://wandb.ai/mwfjord/structural_binding_affinity_predictions_using_gnn/runs/25yve2ja</a><br> View project at: <a href='https://wandb.ai/mwfjord/structural_binding_affinity_predictions_using_gnn' target=\"_blank\">https://wandb.ai/mwfjord/structural_binding_affinity_predictions_using_gnn</a><br>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250207_171140-25yve2ja/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = 10  # You can increase this based on your dataset size\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    \n",
    "    train_loss = train()\n",
    "    val_loss = validate()\n",
    "    \n",
    "    wandb.log({\n",
    "            \"train/loss\": train_loss,\n",
    "            \"val/loss\": val_loss,\n",
    "            \"epoch\": epoch\n",
    "        })\n",
    "    if epoch % 10 == 0 or epoch == 1:\n",
    "        print(f\"Epoch {epoch}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}\")\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"./trained_models/protein_dna_gnn_model.pth\")\n",
    "artifact = wandb.Artifact(name=\"graph_classification_model\", type=\"model\")\n",
    "artifact.add_file(\"./trained_models/protein_dna_gnn_model.pth\")\n",
    "wandb.log_artifact(artifact)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            data = data.to(device)\n",
    "            out = model(data.x, data.edge_index, data.batch)\n",
    "            loss = criterion(out, data.y)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(test_loader)\n",
    "\n",
    "test_loss = test()\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Predictions on New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data):\n",
    "    model.eval()\n",
    "    data = data.to(device)\n",
    "    with torch.no_grad():\n",
    "        out = model(data.x, data.edge_index, data.batch)\n",
    "    return out.item()\n",
    "\n",
    "# Example prediction on a single protein-DNA complex\n",
    "sample_data = test_dataset[0]  # Take the first test sample\n",
    "predicted_affinity = predict(sample_data)\n",
    "print(f\"Predicted Binding Affinity: {predicted_affinity:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
